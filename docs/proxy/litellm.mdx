---
title: LiteLLM
description: "Integrate VibeKit Proxy with LiteLLM for unified access to 100+ LLM providers"
---

## Overview

VibeKit Proxy seamlessly integrates with LiteLLM, a unified interface for calling 100+ LLM APIs in OpenAI format. This integration allows you to route requests through VibeKit Proxy to a LiteLLM instance for enhanced model management and provider abstraction.

<Warning>
Make sure you have the VibeKit Proxy server installed and running before using this integration. See the [Quickstart guide](/proxy/quickstart) for installation instructions.
</Warning>

## Prerequisites

Ensure you have LiteLLM installed and running:

```bash
# Install LiteLLM
pip install litellm[proxy]

# Start LiteLLM proxy
litellm --port 4000
```

## VibeKit Proxy Configuration

Configure VibeKit Proxy to forward requests to your LiteLLM instance:

### Basic LiteLLM Integration

```yaml
# vibekit.yaml
models:
  # Forward all OpenAI-compatible requests to LiteLLM
  - model_name: "gpt-5"
    provider: "openai"
    api_base: "http://localhost:4000"
  
  - model_name: "claude-4-sonnet"
    provider: "openai"  # LiteLLM uses OpenAI-compatible format
    api_base: "http://localhost:4000"
  
  - model_name: "gemini-2.0-flash"
    provider: "openai"  # LiteLLM uses OpenAI-compatible format
    api_base: "http://localhost:4000"
```

### Production LiteLLM Integration

```yaml
# vibekit.yaml
models:
  # Production LiteLLM instance
  - model_name: "gpt-5"
    provider: "openai"
    api_base: "https://your-litellm-proxy.com"

  - model_name: "claude-4-sonnet"
    provider: "openai"
    api_base: "https://your-litellm-proxy.com"
  
  - model_name: "gemini-2.0-flash"
    provider: "openai"
    api_base: "https://your-litellm-proxy.com"
```