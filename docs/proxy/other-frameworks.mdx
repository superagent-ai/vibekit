---
title: Other Frameworks
description: "Integrate VibeKit Proxy with popular AI frameworks and libraries"
---

## Overview

VibeKit Proxy works with any framework or library that makes HTTP requests to AI providers. Simply configure the proxy URL in your existing setup.

<Warning>
Make sure you have the VibeKit Proxy server installed and running before using this integration. See the [Quickstart guide](/proxy/quickstart) for installation instructions.
</Warning>

## LangChain

<CodeGroup>

```python Python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# OpenAI models
openai_llm = ChatOpenAI(
    model="gpt-4",
    base_url="http://localhost:8080/v1"  # Add this line
)

# Anthropic models
anthropic_llm = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    base_url="http://localhost:8080"  # Add this line
)
```

```typescript TypeScript
import { ChatOpenAI } from "@langchain/openai";
import { ChatAnthropic } from "@langchain/anthropic";

// OpenAI models
const openaiLlm = new ChatOpenAI({
  model: "gpt-4",
  configuration: {
    baseURL: "http://localhost:8080/v1"  // Add this line
  }
});

// Anthropic models  
const anthropicLlm = new ChatAnthropic({
  model: "claude-3-5-sonnet-20241022",
  clientOptions: {
    baseURL: "http://localhost:8080"  // Add this line
  }
});
```

</CodeGroup>

## LlamaIndex

<CodeGroup>

```python Python
from llama_index.llms.openai import OpenAI
from llama_index.llms.anthropic import Anthropic

# OpenAI models
openai_llm = OpenAI(
    model="gpt-4",
    api_base="http://localhost:8080/v1"  # Add this line
)

# Anthropic models
anthropic_llm = Anthropic(
    model="claude-3-5-sonnet-20241022",
    api_base="http://localhost:8080"  # Add this line
)
```

```typescript TypeScript
import { OpenAI } from "llamaindex";

const llm = new OpenAI({
  model: "gpt-4",
  additionalChatOptions: {
    baseURL: "http://localhost:8080/v1"  // Add this line
  }
});
```

</CodeGroup>

## Haystack

```python Python
from haystack.components.generators import OpenAIGenerator
from haystack.utils import Secret

generator = OpenAIGenerator(
    model="gpt-4",
    api_key=Secret.from_token("your-api-key"),
    api_base_url="http://localhost:8080/v1"  # Add this line
)
```

## CrewAI

```python Python
from crewai import Agent, LLM

# OpenAI models
openai_llm = LLM(
    model="openai/gpt-4",
    base_url="http://localhost:8080/v1"  # Add this line
)

# Anthropic models
anthropic_llm = LLM(
    model="anthropic/claude-3-5-sonnet-20241022", 
    base_url="http://localhost:8080"  # Add this line
)

agent = Agent(
    role="Customer Service Agent",
    goal="Help customers securely",
    llm=openai_llm
)
```

## AutoGen

```python Python
from autogen import ConversableAgent

# OpenAI configuration
openai_config = {
    "model": "gpt-4",
    "api_key": "your-api-key",
    "base_url": "http://localhost:8080/v1"  # Add this line
}

agent = ConversableAgent(
    name="assistant",
    llm_config={"config_list": [openai_config]}
)
```

## Guidance

```python Python
import guidance

# Configure OpenAI with proxy
guidance.llm = guidance.llms.OpenAI(
    model="gpt-4",
    api_base="http://localhost:8080/v1"  # Add this line
)
```

## Instructor

<CodeGroup>

```python Python
import instructor
from openai import OpenAI

# Patch OpenAI client with proxy
client = instructor.from_openai(
    OpenAI(base_url="http://localhost:8080/v1")  # Add this line
)
```

```typescript TypeScript
import Instructor from "@instructor-ai/instructor";
import OpenAI from "openai";

const oai = new OpenAI({
  baseURL: "http://localhost:8080/v1"  // Add this line
});

const client = Instructor({ client: oai, mode: "FUNCTIONS" });
```

</CodeGroup>

## LiteLLM

```python Python
import litellm

# Set proxy URL for all LiteLLM calls
litellm.api_base = "http://localhost:8080"

# Use with any supported provider
response = litellm.completion(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}]
)
```

## Generic HTTP Clients

For any framework that uses HTTP clients directly:

<CodeGroup>

```python Python
import requests

# Direct HTTP requests to proxy
response = requests.post(
    "http://localhost:8080/v1/chat/completions",  # Use proxy URL
    headers={
        "Authorization": "Bearer your-api-key",
        "Content-Type": "application/json"
    },
    json={
        "model": "gpt-4",
        "messages": [{"role": "user", "content": "Hello"}]
    }
)
```

```typescript TypeScript
// Direct fetch requests to proxy
const response = await fetch("http://localhost:8080/v1/chat/completions", {
  method: "POST",
  headers: {
    "Authorization": "Bearer your-api-key",
    "Content-Type": "application/json"
  },
  body: JSON.stringify({
    model: "gpt-4",
    messages: [{ role: "user", content: "Hello" }]
  })
});
```

```bash cURL
# Direct cURL requests to proxy
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

</CodeGroup>

## Environment Configuration

Set up proxy URLs for any framework using environment variables:

```bash Environment Variables
# Development
VIBEKIT_PROXY_URL=http://localhost:8080
OPENAI_BASE_URL=http://localhost:8080/v1
ANTHROPIC_BASE_URL=http://localhost:8080

# Production  
VIBEKIT_PROXY_URL=https://your-proxy-domain.com
OPENAI_BASE_URL=https://your-proxy-domain.com/v1
ANTHROPIC_BASE_URL=https://your-proxy-domain.com
```

## Integration Pattern

Most AI frameworks follow this pattern:

1. **Find the configuration option** for base URL, API base, or endpoint
2. **Replace the default URL** with your VibeKit Proxy URL
3. **Keep everything else the same** - API keys, models, parameters

Common configuration parameter names:
- `base_url`, `baseURL`, `base_URL`
- `api_base`, `api_base_url`, `apiBase`
- `endpoint`, `host`, `url`
